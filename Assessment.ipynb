{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1123fe11-ac37-4070-9559-202a36d910e4",
   "metadata": {
    "id": "1123fe11-ac37-4070-9559-202a36d910e4"
   },
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d208e-ff07-44f6-b628-b82315c10ba4",
   "metadata": {
    "id": "f34d208e-ff07-44f6-b628-b82315c10ba4"
   },
   "source": [
    "# 최종 평가\n",
    "\n",
    "## 개요\n",
    "\n",
    "이번 교육 과정 평가의 목표는 거대 모델을 구축하고 실행할 수 있는 능력을 평가하는 것입니다. 기존 코드를 DeepSpeed로 이식하고 액티베이션 체크 포인팅, 혼합 정밀도 훈련, ZeRo 리던던시 옵티마이저 등 다양한 DeepSpeed 기능을 사용할 수 있도록 일련의 구성 파일을 활용해 봅니다.\n",
    "\n",
    "태스크를 포함할 수 있도록 단순화된 코드 베이스인 [minGPT](https://github.com/karpathy/minGPT) 를 의도적으로 선택했습니다. 본 코드는 최대 성능을 제공하지는 않지만 대표적이며 비교적 짧은 시간 내에 코딩 평가를 완료할 수 있도록 하는 Transformer의 최소 구현 형태입니다.\n",
    "\n",
    "이 과제에서 우리는 또 다른 모델인 비전 트랜스포머 모델(ViT)에 대해서도 살펴볼 것입니다. 평가 과제를 진행하기 전에 [코드 예제](minGPT/minGPT/play_image.ipynb)를 미리 검토하십시오. 위의 코드 예제를 자유롭게 실행해 보십시오. 하지만 수렴를 위한 훈련은 상당한 시간이 소요되므로 이 과정을 조기에 끝내고 아래에서 논의하는 코드 마이그레이션에 집중하는 것이 도움이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9d34b-dead-4d67-aee5-2a1c614f8fe6",
   "metadata": {
    "id": "95a9d34b-dead-4d67-aee5-2a1c614f8fe6"
   },
   "source": [
    "## 소개\n",
    "\n",
    "개념적으로 우리의 과제 목표는 다음과 같습니다:\n",
    "- 학습 파이프라인의 독립 실행형 파이토치 구현을 DeepSpeed로 마이그레이션하고 \"2개의 서버\" 클러스터에서 효과적으로 훈련하십시오.\n",
    "- 혼합 정밀도 훈련, 액티베이션 체크 포인팅, 제로 리던던시 옵티마이저와 같은 메모리 절약을 가능하게 하는 기능을 활성화합니다.\n",
    "- 학습 중인 모델의 크기를 늘려봅니다.\n",
    "\n",
    "아래 노트북은 프로세스를 안내하고 테스트 코드를 제공하여 올바른 해결 방법을 모색하는 데 도움이 됩니다. 평가가 끝날 때, 코딩을 완료하면 주피터랩 플랫폼으로 돌아가 'Assess' 버튼을 누릅니다. 버튼을 누르면 코드 파일과 DeepSpeed 구성 파일을 로드하고 이를 실행하여 구현의 정확성을 평가하는 자동화 프로세스가 실행될 예정입니다. 완료하는 데 몇 분 정도 걸릴 수 있으므로 이 단계를 실행할 충분한 시간을 두는 것이 좋습니다. 시간이 촉박한 경우 나중에 완료할 수 있도록 수정한 파일을 다운로드해두는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b959719-1fef-4a5a-8454-5babc9c6aa90",
   "metadata": {
    "id": "9b959719-1fef-4a5a-8454-5babc9c6aa90"
   },
   "source": [
    "## 1단계: 베이스라인 구현\n",
    "\n",
    "평가의 시작 지점인 [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)부터 살펴보겠습니다. 이 파일은 이전에 검토했던 내용과 동일한 코드이며, 배치 실행을 위해 파이썬 파일로 추출되었습니다. 독립 실행형 모드에서 작동하는지 테스트해 보겠습니다. 다시 한 번 말하지만, 수렴을 위한 훈련에는 상당한 시간이 소요되므로, 일단 훈련 진행되는 것이 확인되면 자유롭게 훈련 과정을 중단하고 다음 단계로 넘어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf98ec2-b1b8-4a6c-b2de-8410ea4562cf",
   "metadata": {
    "id": "acf98ec2-b1b8-4a6c-b2de-8410ea4562cf",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "170499072it [00:09, 17579636.64it/s]                                            \n",
      "Extracting ./cifar-10-python.tar.gz to ./\n",
      "Files already downloaded and verified\n",
      "50000 10000\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "05/13/2024 10:52:55 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "  0%|                                                 | 0/50000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python minGPT/minGPT/runStartingPoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae562f-1b8c-4383-a83d-101983f70ea7",
   "metadata": {
    "id": "19ae562f-1b8c-4383-a83d-101983f70ea7"
   },
   "source": [
    "## 2단계 : DeepSpeed 활성화\n",
    "\n",
    "먼저 코드 변경을 최소화하고 DeepSpeed 라이브러리를 사용하도록 이전 훈련 스크립트를 수정하는 것으로 시작하겠습니다. 이 작업을 위해서는 다음이 필요합니다:\n",
    "\n",
    "&nbsp; &nbsp; 1.  [runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py)  에서 관련되는 부분을 수정합니다.   \n",
    "&nbsp; &nbsp;  2.  [trainer.py](./minGPT/minGPT/mingpt/trainer.py) 에서 관련되는 부분을 수정합니다.   \n",
    "&nbsp; &nbsp;  3.  DeepSpeed 구성 파일 `ds_config_basic.json`   을 생성합니다.   \n",
    "&nbsp; &nbsp;  4. `deepspeed` 명령어를 사용하여 트레이닝을 실행합니다.\n",
    "\n",
    "\n",
    "### 1. `runFirstDeepSpeed.py` 파일의 \"ToDo Step 2\" 섹션을 수정합니다.\n",
    "[runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py) 파일을 열고 DeepSpeed에 코드를 이식할 \"ToDo Step 2\" 섹션을 정의합니다. 정의해야 할 섹션은 총 4개입니다.\n",
    "\n",
    "### 2.  `trainer.py` 파일에서 \"ToDo Step 2\" 섹션을 수정합니다.\n",
    "[trainer.py](./minGPT/minGPT/mingpt/trainer.py) 파일을 열고 \"ToDo Step 2\" 섹션을 정의하여 `DeepSpeedTrainer` 클래스를 구현합니다. 수정/구현할 섹션은 총 6개입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba61b3-6626-4ca2-b38c-408dbf57a99f",
   "metadata": {
    "id": "5bba61b3-6626-4ca2-b38c-408dbf57a99f"
   },
   "source": [
    "### 3. DeepSpeed 구성 파일 `ds_config_basic.json`을 생성합니다.\n",
    "다음 셀에서 `FIXME`를 다음과 같이 수정합니다.\n",
    "- GPU당 마이크로 배치 크기는 8입니다.\n",
    "- Adam optimizer를 활성화하고 원래 코드 [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)에서 학습률을 복사해야 합니다.\n",
    "- 그래디언트 클리핑을 원래 코드  [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)에 사용된 값으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9eaaaf7-6af1-44d4-8eba-8143e81dcfb3",
   "metadata": {
    "id": "c9eaaaf7-6af1-44d4-8eba-8143e81dcfb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/ds_config_basic.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/ds_config_basic.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 8,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-3\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc199bd-2670-4878-be02-f65a9273540a",
   "metadata": {
    "id": "ddc199bd-2670-4878-be02-f65a9273540a"
   },
   "source": [
    "### 4.`deepspeed` 명령어로 트레이닝을 실행합니다.\n",
    "\n",
    "다음 명령어는 4개의 GPU 트레이닝을 진행해야 하며 훈련이 진행되는 것을 확인해야 합니다. 다시 한 번 말하지만, 이 연습의 목표는 이 모델을 수렴하도록 훈련시키는 것이 아닙니다. 훈련이 실행 중이면 실행을 중단하고 다음 단계로 이동할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed7e3b8-820b-4757-b0f0-2d6ad801418f",
   "metadata": {
    "id": "8ed7e3b8-820b-4757-b0f0-2d6ad801418f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 11:04:57,026] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-05-13 11:04:57,190] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config minGPT/minGPT/ds_config_basic.json\n",
      "[2024-05-13 11:04:58,664] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "[2024-05-13 11:04:58,664] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "[2024-05-13 11:04:58,665] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-05-13 11:04:58,665] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-05-13 11:04:58,665] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-05-13 11:04:58,665] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2024-05-13 11:04:58,665] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-05-13 11:05:00,379] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "05/13/2024 11:05:36 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "05/13/2024 11:05:36 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "05/13/2024 11:05:37 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "05/13/2024 11:05:37 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "[2024-05-13 11:05:37,357] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "05/13/2024 11:05:39 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "[2024-05-13 11:05:40,279] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Creating extension directory /home/admin/.cache/torch_extensions/py38_cu115/fused_adam...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -std=c++14 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 27.902531623840332 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Creating extension directory /home/admin/.cache/torch_extensions/py38_cu115/utils...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 27.94715690612793 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 27.946157693862915 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 27.955899238586426 seconds\n",
      "[2024-05-13 11:06:08,965] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-05-13 11:06:08,973] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-05-13 11:06:08,974] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-05-13 11:06:08,974] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2024-05-13 11:06:08,974] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-05-13 11:06:08,974] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\n",
      "[2024-05-13 11:06:08,974] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-13 11:06:08,975] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   fp16_enabled ................. False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-13 11:06:08,976] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.003}\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "[2024-05-13 11:06:08,977] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   train_batch_size ............. 32\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "[2024-05-13 11:06:08,978] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2024-05-13 11:06:08,979] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "[2024-05-13 11:06:08,979] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "[2024-05-13 11:06:08,979] [INFO] [config.py:1065:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.003\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0\n",
      "}\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /opt/conda/lib/python3.8/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.802481889724731 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.824244976043701 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.824045658111572 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.823307275772095 seconds\n",
      "epoch 1 iter 9: train loss 6.25524. lr 4.800000e-06:   0%| | 9/12500 [00:03<56:0[2024-05-13 11:06:27,514] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.32e-06], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 9: train loss 6.25524. lr 4.800000e-06:   0%| | 10/12500 [00:03<54:[2024-05-13 11:06:27,515] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=129.08259641987368, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 18: train loss 6.19819. lr 9.120000e-06:   0%| | 19/12500 [00:05<51[2024-05-13 11:06:29,984] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.12e-06], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 19: train loss 6.21944. lr 9.600000e-06:   0%| | 19/12500 [00:05<51[2024-05-13 11:06:29,985] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=129.54218355916305, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 28: train loss 6.11083. lr 1.392000e-05:   0%| | 29/12500 [00:08<51[2024-05-13 11:06:32,451] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.392e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 29: train loss 6.15623. lr 1.440000e-05:   0%| | 29/12500 [00:08<51[2024-05-13 11:06:32,452] [INFO] [timer.py:193:stop] 0/30, SamplesPerSec=129.7345737361957, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 38: train loss 6.15250. lr 1.872000e-05:   0%| | 39/12500 [00:10<51[2024-05-13 11:06:34,915] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.872e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 39: train loss 6.18284. lr 1.920000e-05:   0%| | 39/12500 [00:11<51[2024-05-13 11:06:34,916] [INFO] [timer.py:193:stop] 0/40, SamplesPerSec=129.86187369323952, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 48: train loss 6.11005. lr 2.352000e-05:   0%| | 49/12500 [00:13<51[2024-05-13 11:06:37,390] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.352e-05], mom=[(0.9, 0.999)]\n",
      "epoch 1 iter 49: train loss 6.13187. lr 2.400000e-05:   0%| | 50/12500 [00:13<51[2024-05-13 11:06:37,391] [INFO] [timer.py:193:stop] 0/50, SamplesPerSec=129.82082119765573, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "epoch 1 iter 52: train loss 6.07145. lr 2.544000e-05:   0%| | 53/12500 [00:14<51^C\n",
      "[2024-05-13 11:06:38,327] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 614\n",
      "[2024-05-13 11:06:38,327] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 615\n",
      "[2024-05-13 11:06:38,328] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 616\n",
      "[2024-05-13 11:06:38,328] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 617\n",
      "[2024-05-13 11:06:38,328] [INFO] [launch.py:187:sigkill_handler] Main process received SIGINT, exiting\n"
     ]
    }
   ],
   "source": [
    "!deepspeed minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config minGPT/minGPT/ds_config_basic.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ed01e-9ca5-441c-9275-d4573115d1b7",
   "metadata": {
    "id": "583ed01e-9ca5-441c-9275-d4573115d1b7"
   },
   "source": [
    "## 3단계: 멀티 노드 실행\n",
    "\n",
    "위 코드는 이 특정 노드에 대해 4개의 GPU에서 실행되었지만, 우리의 목표는 수업 초반에 사용한 두개의 노드에서 작동하도록 만드는 것입니다. 앞서 작업한 코드를 다시 사용하여 위의 두개의 노드에서 실행되도록 작업 코드를 수정하세요. 적절한 Shell 스크립트를 생성하는 것부터 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ab689f-eb64-4881-a7da-fbf963bd7721",
   "metadata": {
    "id": "62ab689f-eb64-4881-a7da-fbf963bd7721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep3.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep3.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step3\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runFirstDeepSpeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config minGPT/minGPT/ds_config_basic.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83193290-4079-4a9d-a1b0-17f939b45d29",
   "metadata": {
    "id": "83193290-4079-4a9d-a1b0-17f939b45d29"
   },
   "source": [
    "멀티 노드 실행을 활성화하려면 아래를 수정하십시오. 아래 명령어를 사용하여 멀티 노드 작업을 실행하십시오 (평가 제출 시 사용할 명령어이므로 파일 이름이나 경로를 변경하지 마십시오)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fae3988-8796-4c1c-873a-f4828c90310c",
   "metadata": {
    "id": "7fae3988-8796-4c1c-873a-f4828c90310c",
    "outputId": "990eb47c-3f89-42b5-dbeb-b54e9745e804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 2  slurmpar dli_asse    admin  R       0:01      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!sbatch ./minGPT/minGPT/runSlurmStep3.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3d948-32ac-4043-aa99-1b0ce50108a8",
   "metadata": {
    "id": "a3d3d948-32ac-4043-aa99-1b0ce50108a8"
   },
   "source": [
    "위 명령어가 실행되면 아래 명령어로 출력 및 오류 로그를 볼 수 있습니다. 작업 ID를 아래 명령어로 복사하십시오. 다시 한 번 코드 배포 시 아래 파일 위치에 다음과 같은 파일명 구조로 로그를 남기는 지 확인하십시오. 동일하게 평가 제출 시에도 해당 로그 파일들에 대한 확인이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e584a73-c60d-43c0-bd6c-6c2a4725ae83",
   "metadata": {
    "id": "6e584a73-c60d-43c0-bd6c-6c2a4725ae83",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 11:06:50,772] [INFO] [runner.py:378:main] Using IP address of 172.18.0.5 for node slurmnode1\n",
      "[2024-05-13 11:06:50,773] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2024-05-13 11:06:50,773] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_P2P_DISABLE=1; export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.5 --master_port=29500 /dli/minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config 'minGPT/minGPT/ds_config_basic.json'\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:96:main] 1 NCCL_P2P_DISABLE=1\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2024-05-13 11:06:53,041] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-05-13 11:06:53,045] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "slurmnode1: [2024-05-13 11:06:53,046] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2024-05-13 11:06:53,046] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2024-05-13 11:06:53,046] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2024-05-13 11:06:53,046] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2024-05-13 11:06:53,046] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2024-05-13 11:06:53,046] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-05-13 11:06:54,755] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "slurmnode1: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "slurmnode2: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "slurmnode2: Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n",
      "slurmnode1: Extracting ./cifar-10-python.tar.gz to ./\n",
      "slurmnode1: Extracting ./cifar-10-python.tar.gz to ./\n",
      "slurmnode2: Extracting ./cifar-10-python.tar.gz to ./\n",
      "slurmnode2: Extracting ./cifar-10-python.tar.gz to ./\n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-05-13 11:07:46,165] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-05-13 11:07:50,379] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: \n",
      "slurmnode1: Detected CUDA files, patching ldflags\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "slurmnode1: Building extension module fused_adam...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.09494662284851074 seconds\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.1020972728729248 secondsTime to load fused_adam op: 0.10210108757019043 seconds\n",
      "slurmnode2: \n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode1: Building extension module utils...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.10224390029907227 seconds\n",
      "slurmnode1: [2024-05-13 11:07:51,267] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.0879664421081543 seconds\n",
      "slurmnode1: [2024-05-13 11:07:51,276] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "slurmnode1: [2024-05-13 11:07:51,276] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2024-05-13 11:07:51,276] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2024-05-13 11:07:51,276] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2024-05-13 11:07:51,276] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:07:51,277] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2024-05-13 11:07:51,277] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": false, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": false, \n",
      "slurmnode1:     \"cpu_checkpointing\": false, \n",
      "slurmnode1:     \"number_checkpoints\": null, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": false, \n",
      "slurmnode1:     \"profile\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:07:51,277] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2024-05-13 11:07:51,277] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2024-05-13 11:07:51,277] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2024-05-13 11:07:51,278] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   fp16_enabled ................. False\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2024-05-13 11:07:51,279] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.003}\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2024-05-13 11:07:51,280] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   train_batch_size ............. 32\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2024-05-13 11:07:51,281] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 0, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": false, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": null, \n",
      "slurmnode1:     \"offload_optimizer\": null, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+07, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:07:51,282] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "slurmnode1: [2024-05-13 11:07:51,282] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.10189580917358398 seconds\n",
      "slurmnode2: Time to load utils op: 0.10166501998901367 seconds\n",
      "slurmnode1: [2024-05-13 11:07:51,366] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 8, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1.0\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode1: Building extension module utils...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.1701192855834961 seconds\n",
      "slurmnode1: [2024-05-13 11:07:55,324] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[4.32e-06], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:07:55,325] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=121.98228014251956, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:07:57,894] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.12e-06], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:07:57,894] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=123.54911123996014, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:00,462] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=0, lr=[1.392e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:00,462] [INFO] [timer.py:193:stop] 0/30, SamplesPerSec=124.03515907216688, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:03,105] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=0, lr=[1.872e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:03,105] [INFO] [timer.py:193:stop] 0/40, SamplesPerSec=123.32176340659299, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:05,761] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=0, lr=[2.352e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:05,762] [INFO] [timer.py:193:stop] 0/50, SamplesPerSec=122.77926858027054, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:08,435] [INFO] [logging.py:69:log_dist] [Rank 0] step=60, skipped=0, lr=[2.8320000000000003e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:08,436] [INFO] [timer.py:193:stop] 0/60, SamplesPerSec=122.28467113497501, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:11,026] [INFO] [logging.py:69:log_dist] [Rank 0] step=70, skipped=0, lr=[3.312e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:11,027] [INFO] [timer.py:193:stop] 0/70, SamplesPerSec=122.50339490757824, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:13,610] [INFO] [logging.py:69:log_dist] [Rank 0] step=80, skipped=0, lr=[3.792e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:13,610] [INFO] [timer.py:193:stop] 0/80, SamplesPerSec=122.71563553323637, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:16,192] [INFO] [logging.py:69:log_dist] [Rank 0] step=90, skipped=0, lr=[4.272e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:16,193] [INFO] [timer.py:193:stop] 0/90, SamplesPerSec=122.88268045619643, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:18,775] [INFO] [logging.py:69:log_dist] [Rank 0] step=100, skipped=0, lr=[4.752e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:18,775] [INFO] [timer.py:193:stop] 0/100, SamplesPerSec=123.01771449872344, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:21,359] [INFO] [logging.py:69:log_dist] [Rank 0] step=110, skipped=0, lr=[5.232e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:21,360] [INFO] [timer.py:193:stop] 0/110, SamplesPerSec=123.11745066794344, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:23,943] [INFO] [logging.py:69:log_dist] [Rank 0] step=120, skipped=0, lr=[5.712e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:23,943] [INFO] [timer.py:193:stop] 0/120, SamplesPerSec=123.20678560063511, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:26,526] [INFO] [logging.py:69:log_dist] [Rank 0] step=130, skipped=0, lr=[6.192e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:26,527] [INFO] [timer.py:193:stop] 0/130, SamplesPerSec=123.28054307860769, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:29,108] [INFO] [logging.py:69:log_dist] [Rank 0] step=140, skipped=0, lr=[6.672e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:29,109] [INFO] [timer.py:193:stop] 0/140, SamplesPerSec=123.3484486237752, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:31,691] [INFO] [logging.py:69:log_dist] [Rank 0] step=150, skipped=0, lr=[7.152e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:31,692] [INFO] [timer.py:193:stop] 0/150, SamplesPerSec=123.40557367693836, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:34,276] [INFO] [logging.py:69:log_dist] [Rank 0] step=160, skipped=0, lr=[7.632e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:34,276] [INFO] [timer.py:193:stop] 0/160, SamplesPerSec=123.45095312334726, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:36,878] [INFO] [logging.py:69:log_dist] [Rank 0] step=170, skipped=0, lr=[8.112000000000001e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:36,878] [INFO] [timer.py:193:stop] 0/170, SamplesPerSec=123.43983927393309, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:39,461] [INFO] [logging.py:69:log_dist] [Rank 0] step=180, skipped=0, lr=[8.592e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:39,461] [INFO] [timer.py:193:stop] 0/180, SamplesPerSec=123.48249304147247, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:42,044] [INFO] [logging.py:69:log_dist] [Rank 0] step=190, skipped=0, lr=[9.072e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:42,044] [INFO] [timer.py:193:stop] 0/190, SamplesPerSec=123.52041644500561, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:44,626] [INFO] [logging.py:69:log_dist] [Rank 0] step=200, skipped=0, lr=[9.552000000000001e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:44,627] [INFO] [timer.py:193:stop] 0/200, SamplesPerSec=123.55454115360149, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:47,208] [INFO] [logging.py:69:log_dist] [Rank 0] step=210, skipped=0, lr=[0.00010031999999999999], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:47,209] [INFO] [timer.py:193:stop] 0/210, SamplesPerSec=123.58698909832052, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:49,835] [INFO] [logging.py:69:log_dist] [Rank 0] step=220, skipped=0, lr=[0.00010512000000000001], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:49,835] [INFO] [timer.py:193:stop] 0/220, SamplesPerSec=123.52042985288088, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:52,418] [INFO] [logging.py:69:log_dist] [Rank 0] step=230, skipped=0, lr=[0.00010992], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:52,418] [INFO] [timer.py:193:stop] 0/230, SamplesPerSec=123.54935848358616, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:55,000] [INFO] [logging.py:69:log_dist] [Rank 0] step=240, skipped=0, lr=[0.00011472000000000001], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:55,000] [INFO] [timer.py:193:stop] 0/240, SamplesPerSec=123.5778596508512, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:08:57,582] [INFO] [logging.py:69:log_dist] [Rank 0] step=250, skipped=0, lr=[0.00011952000000000001], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:08:57,583] [INFO] [timer.py:193:stop] 0/250, SamplesPerSec=123.60340381811422, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:00,165] [INFO] [logging.py:69:log_dist] [Rank 0] step=260, skipped=0, lr=[0.00012432], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:00,165] [INFO] [timer.py:193:stop] 0/260, SamplesPerSec=123.62664836786296, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:02,749] [INFO] [logging.py:69:log_dist] [Rank 0] step=270, skipped=0, lr=[0.00012912], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:02,750] [INFO] [timer.py:193:stop] 0/270, SamplesPerSec=123.64530345547473, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:05,333] [INFO] [logging.py:69:log_dist] [Rank 0] step=280, skipped=0, lr=[0.00013392], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:05,334] [INFO] [timer.py:193:stop] 0/280, SamplesPerSec=123.6632034743141, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:07,937] [INFO] [logging.py:69:log_dist] [Rank 0] step=290, skipped=0, lr=[0.00013872], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:07,937] [INFO] [timer.py:193:stop] 0/290, SamplesPerSec=123.6472234653413, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:10,517] [INFO] [logging.py:69:log_dist] [Rank 0] step=300, skipped=0, lr=[0.00014352], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:10,517] [INFO] [timer.py:193:stop] 0/300, SamplesPerSec=123.67070437841087, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:13,097] [INFO] [logging.py:69:log_dist] [Rank 0] step=310, skipped=0, lr=[0.00014832], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:13,097] [INFO] [timer.py:193:stop] 0/310, SamplesPerSec=123.69188326451078, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:15,678] [INFO] [logging.py:69:log_dist] [Rank 0] step=320, skipped=0, lr=[0.00015312], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:15,678] [INFO] [timer.py:193:stop] 0/320, SamplesPerSec=123.71083813778615, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:18,257] [INFO] [logging.py:69:log_dist] [Rank 0] step=330, skipped=0, lr=[0.00015792], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:18,257] [INFO] [timer.py:193:stop] 0/330, SamplesPerSec=123.73126315264663, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:20,840] [INFO] [logging.py:69:log_dist] [Rank 0] step=340, skipped=0, lr=[0.00016272], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:20,841] [INFO] [timer.py:193:stop] 0/340, SamplesPerSec=123.74413835499196, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:23,420] [INFO] [logging.py:69:log_dist] [Rank 0] step=350, skipped=0, lr=[0.00016752], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:23,421] [INFO] [timer.py:193:stop] 0/350, SamplesPerSec=123.76111215822446, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:25,991] [INFO] [logging.py:69:log_dist] [Rank 0] step=360, skipped=0, lr=[0.00017232], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:25,992] [INFO] [timer.py:193:stop] 0/360, SamplesPerSec=123.78903850169787, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:28,561] [INFO] [logging.py:69:log_dist] [Rank 0] step=370, skipped=0, lr=[0.00017712], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:28,562] [INFO] [timer.py:193:stop] 0/370, SamplesPerSec=123.81674989502665, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:31,132] [INFO] [logging.py:69:log_dist] [Rank 0] step=380, skipped=0, lr=[0.00018192], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:31,133] [INFO] [timer.py:193:stop] 0/380, SamplesPerSec=123.84171207467905, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:33,723] [INFO] [logging.py:69:log_dist] [Rank 0] step=390, skipped=0, lr=[0.00018672], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:33,724] [INFO] [timer.py:193:stop] 0/390, SamplesPerSec=123.84094224407923, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:36,316] [INFO] [logging.py:69:log_dist] [Rank 0] step=400, skipped=0, lr=[0.00019151999999999998], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:36,317] [INFO] [timer.py:193:stop] 0/400, SamplesPerSec=123.83808575607749, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:38,905] [INFO] [logging.py:69:log_dist] [Rank 0] step=410, skipped=0, lr=[0.00019632], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:38,906] [INFO] [timer.py:193:stop] 0/410, SamplesPerSec=123.83930017404538, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:41,505] [INFO] [logging.py:69:log_dist] [Rank 0] step=420, skipped=0, lr=[0.00020112], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:41,506] [INFO] [timer.py:193:stop] 0/420, SamplesPerSec=123.82871096251384, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:44,101] [INFO] [logging.py:69:log_dist] [Rank 0] step=430, skipped=0, lr=[0.00020592000000000003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:44,101] [INFO] [timer.py:193:stop] 0/430, SamplesPerSec=123.82300354283518, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:46,696] [INFO] [logging.py:69:log_dist] [Rank 0] step=440, skipped=0, lr=[0.00021072], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:46,696] [INFO] [timer.py:193:stop] 0/440, SamplesPerSec=123.81831611180793, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:49,287] [INFO] [logging.py:69:log_dist] [Rank 0] step=450, skipped=0, lr=[0.00021552], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:49,288] [INFO] [timer.py:193:stop] 0/450, SamplesPerSec=123.81739463509415, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:51,876] [INFO] [logging.py:69:log_dist] [Rank 0] step=460, skipped=0, lr=[0.00022032000000000003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:51,877] [INFO] [timer.py:193:stop] 0/460, SamplesPerSec=123.81927249085571, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:54,479] [INFO] [logging.py:69:log_dist] [Rank 0] step=470, skipped=0, lr=[0.00022511999999999999], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:54,479] [INFO] [timer.py:193:stop] 0/470, SamplesPerSec=123.80709444831137, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:57,071] [INFO] [logging.py:69:log_dist] [Rank 0] step=480, skipped=0, lr=[0.00022992], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:57,072] [INFO] [timer.py:193:stop] 0/480, SamplesPerSec=123.80591284594051, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:09:59,665] [INFO] [logging.py:69:log_dist] [Rank 0] step=490, skipped=0, lr=[0.00023472000000000003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:09:59,666] [INFO] [timer.py:193:stop] 0/490, SamplesPerSec=123.80293895043208, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:02,250] [INFO] [logging.py:69:log_dist] [Rank 0] step=500, skipped=0, lr=[0.00023951999999999998], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:02,250] [INFO] [timer.py:193:stop] 0/500, SamplesPerSec=123.80902128946006, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:04,838] [INFO] [logging.py:69:log_dist] [Rank 0] step=510, skipped=0, lr=[0.00024432], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:04,839] [INFO] [timer.py:193:stop] 0/510, SamplesPerSec=123.81137195113394, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:07,432] [INFO] [logging.py:69:log_dist] [Rank 0] step=520, skipped=0, lr=[0.00024912000000000003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:07,433] [INFO] [timer.py:193:stop] 0/520, SamplesPerSec=123.80860266810232, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:10,018] [INFO] [logging.py:69:log_dist] [Rank 0] step=530, skipped=0, lr=[0.00025392000000000004], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:10,019] [INFO] [timer.py:193:stop] 0/530, SamplesPerSec=123.81300374285276, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:12,607] [INFO] [logging.py:69:log_dist] [Rank 0] step=540, skipped=0, lr=[0.00025872], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:12,608] [INFO] [timer.py:193:stop] 0/540, SamplesPerSec=123.8146464959608, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:15,193] [INFO] [logging.py:69:log_dist] [Rank 0] step=550, skipped=0, lr=[0.00026352], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:15,194] [INFO] [timer.py:193:stop] 0/550, SamplesPerSec=123.81889940922018, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:17,779] [INFO] [logging.py:69:log_dist] [Rank 0] step=560, skipped=0, lr=[0.00026832], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:17,780] [INFO] [timer.py:193:stop] 0/560, SamplesPerSec=123.82277395067221, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:20,368] [INFO] [logging.py:69:log_dist] [Rank 0] step=570, skipped=0, lr=[0.00027312], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:20,368] [INFO] [timer.py:193:stop] 0/570, SamplesPerSec=123.8246899150257, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:22,946] [INFO] [logging.py:69:log_dist] [Rank 0] step=580, skipped=0, lr=[0.00027792], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:22,946] [INFO] [timer.py:193:stop] 0/580, SamplesPerSec=123.83493406958773, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:25,529] [INFO] [logging.py:69:log_dist] [Rank 0] step=590, skipped=0, lr=[0.00028272000000000003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:25,530] [INFO] [timer.py:193:stop] 0/590, SamplesPerSec=123.84070776957755, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:28,123] [INFO] [logging.py:69:log_dist] [Rank 0] step=600, skipped=0, lr=[0.00028752], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:28,124] [INFO] [timer.py:193:stop] 0/600, SamplesPerSec=123.83750551780011, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:30,713] [INFO] [logging.py:69:log_dist] [Rank 0] step=610, skipped=0, lr=[0.00029232], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:30,714] [INFO] [timer.py:193:stop] 0/610, SamplesPerSec=123.83787369639558, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:33,286] [INFO] [logging.py:69:log_dist] [Rank 0] step=620, skipped=0, lr=[0.00029712], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:33,287] [INFO] [timer.py:193:stop] 0/620, SamplesPerSec=123.85153674484249, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:35,860] [INFO] [logging.py:69:log_dist] [Rank 0] step=630, skipped=0, lr=[0.00030192], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:35,860] [INFO] [timer.py:193:stop] 0/630, SamplesPerSec=123.86403180482311, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n",
      "slurmnode1: [2024-05-13 11:10:38,430] [INFO] [logging.py:69:log_dist] [Rank 0] step=640, skipped=0, lr=[0.00030672], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:10:38,430] [INFO] [timer.py:193:stop] 0/640, SamplesPerSec=123.8788036314236, MemAllocated=0.17GB, MaxMemAllocated=8.84GB\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=2;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83fe2de0-5d66-4880-ab10-074bc896e1ae",
   "metadata": {
    "id": "83fe2de0-5d66-4880-ab10-074bc896e1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode2: Warning: Permanently added 'slurmnode2,172.18.0.3' (ECDSA) to the list of known hosts.\n",
      "170499072it [00:12, 13613113.83it/s]                               \n",
      "170499072it [00:12, 13587490.53it/s]                               \n",
      "170499072it [00:12, 13584892.34it/s]                               \n",
      "170499072it [00:12, 13583342.32it/s]                               \n",
      "slurmnode1: 05/13/2024 11:07:46 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 05/13/2024 11:07:46 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 05/13/2024 11:07:46 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 05/13/2024 11:07:46 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "slurmnode1: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode2: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "slurmnode1: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode1: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 05/13/2024 11:07:49 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=2;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cbe44-51ad-4411-8ef5-792075915855",
   "metadata": {
    "id": "ee6cbe44-51ad-4411-8ef5-792075915855"
   },
   "source": [
    "코드가 만족스러우면 다음 단계로 이동하기 전에 배치 작업이 종료되었는지 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c2e59b2-a040-40e7-bb47-022eba1f15d3",
   "metadata": {
    "id": "3c2e59b2-a040-40e7-bb47-022eba1f15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 2  slurmpar dli_asse    admin  R       4:13      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a98f6482-d538-4e28-b299-5bc0e3824cc3",
   "metadata": {
    "id": "a98f6482-d538-4e28-b299-5bc0e3824cc3"
   },
   "outputs": [],
   "source": [
    "!scancel 2 #PASTE_JOB_ID_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18320d-6f90-41b5-93ea-b2c6afc36317",
   "metadata": {
    "id": "1c18320d-6f90-41b5-93ea-b2c6afc36317"
   },
   "source": [
    "## 4단계: 코드를 추가로 개선하기\n",
    "\n",
    "현재 우리는 활성화 체크포인팅을 수행할 수 있는 기능이 없습니다. 이번 단계에서는 DeepSpeed 라이브러리로 활성화 체크포인팅을 실행할 수 있는 코드를 소개합니다.\n",
    "\n",
    "&nbsp; &nbsp;  1. 활성화 체크포인팅을 위한 트랜스포머 블록을 정의합니다.   \n",
    "&nbsp; &nbsp;  2. 활성화 체크포인팅 및 FP16 트레이닝을 지원하는 DeepSeed 구성 파일을 만듭니다.   \n",
    "&nbsp; &nbsp;  3.  sbatch 트레이닝 파일을 만들고 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1cd8f",
   "metadata": {
    "id": "b7c1cd8f"
   },
   "source": [
    "### 1. 활성화 체크포인팅을 위한 트랜스포머 블록을 정의합니다.\n",
    "\n",
    "DeepSpeed를 사용하여 모델(또는 모델의 일부)의 활성화 체크포인팅을 활성화하려면 포워드 패스 정의에서 각 블록을 `deepspeed.checkpointing.checkpoint()` ([더 알아보기](https://deepspeed.readthedocs.io/en/stable/activation-checkpointing.html#deepspeed.checkpointing.checkpoint)) 함수로 감싸야 합니다.\n",
    "\n",
    "아래 예제는 CNN 블록 2개와 DeepSpeed로 활성화 체크포인팅을 위해CNN 블록을 감싸는 선형 레이어가 있는 간단한 합성곱 신경망 정의를 보여줍니다.\n",
    "\n",
    "```\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_block_1 = nn.Sequential(*[nn.Conv2d(3, 32, 3, padding=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2)])\n",
    "        self.cnn_block_2 = nn.Sequential(*[nn.Conv2d(64, 64, 3, padding=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2)])\n",
    "        self.flatten = lambda inp: torch.flatten(inp, 1)\n",
    "        self.linearize = nn.Sequential(*[ nn.Linear(64 * 8 * 8, 512),nn.ReLU()])\n",
    "        self.out = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = deepspeed.checkpointing.checkpoint(self.cnn_block_1, X)\n",
    "        X = deepspeed.checkpointing.checkpoint(self.cnn_block_2, X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.linearize(X)\n",
    "        X = self.out(X)\n",
    "        return X\n",
    "\n",
    "```\n",
    "\n",
    "비슷한 메커니즘은 `torch.utils.checkpoint.checkpoint()`기능을 통해 토치와 함께 구현됩니다.\n",
    "\n",
    "\n",
    "우리의 경우, Vision Transformer 모델은 `./minGPT/minGPT/mingpt/model.py` 파일의 GPT 클래스로 구현됩니다. DeepSpeed 활성화 체크포인팅으로 트랜스포머 블록을 감싸야 합니다.  [model.py](./minGPT/minGPT/mingpt/model.py) 파일에서 \"Step 4 ToDo\" 태스크를 수정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533ffd5",
   "metadata": {
    "id": "3533ffd5"
   },
   "source": [
    "### 2. DeepSeed 구성 파일을 만듭니다.\n",
    "\n",
    "시작하기 전에 config-json 파일의 DeepSpeed 문서에서 [activation-checkpointing](https://www.deepspeed.ai/docs/config-json/#activation-checkpointing) 을 확인해볼 수 있습니다.\n",
    "\n",
    "아래 셀의 `#FIXME`를 다음과 같이 수정하여 `ds_config_step4.json` 을 만듭니다.\n",
    "- 활성화 체크포인팅을 활성화합니다.\n",
    "- 활성화 체크포인팅이 제대로 작동하는지 확인하려면 GPU당 마이크로 배치 크기를 128로 설정하십시오.\n",
    "- 체크포인트 수를 12개로 만듭니다.\n",
    "- FP16 트레이닝을 활성화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0112af26",
   "metadata": {
    "id": "0112af26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minGPT/minGPT/ds_config_step4.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile minGPT/minGPT/ds_config_step4.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 128,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "  \"activation_checkpointing\": {\n",
    "    \"number_checkpoints\": 12\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f43cef",
   "metadata": {
    "id": "72f43cef"
   },
   "source": [
    "### 3. sbatch 트레이닝 파일을 실행합니다.\n",
    "\n",
    "\n",
    "먼저 훈련용 파이썬 스크립트`runFirstDeepSpeed.py`의 복사본을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa3bad5d",
   "metadata": {
    "id": "aa3bad5d"
   },
   "outputs": [],
   "source": [
    "!cp /dli/minGPT/minGPT/runFirstDeepSpeed.py /dli/minGPT/minGPT/runStep4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37d3a0",
   "metadata": {
    "id": "af37d3a0"
   },
   "source": [
    "이제 sbatch 파일 `runSlurmStep4.sh`을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7efe061f",
   "metadata": {
    "id": "7efe061f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep4.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep4.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step4\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runStep4.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/minGPT/minGPT/ds_config_step4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb16851",
   "metadata": {
    "id": "0cb16851"
   },
   "source": [
    "위의 작업을 완료한 후 아래 명령어를 실행하여 훈련 작업을 슬럼 스케줄러에 제출하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8a10d38-9eda-41e2-bcce-8156caced370",
   "metadata": {
    "id": "e8a10d38-9eda-41e2-bcce-8156caced370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 4\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 4  slurmpar dli_asse    admin  R       0:00      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!sbatch /dli/minGPT/minGPT/runSlurmStep4.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7261f-bd99-4cea-98b6-5be237aa3446",
   "metadata": {
    "id": "59e7261f-bd99-4cea-98b6-5be237aa3446"
   },
   "source": [
    "아래를 사용하여 코드 실행을 확인합니다 (배치 크기가 크더라도 진행이 확인되어야 합니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e3bf8a5-c6f8-4e16-89ea-b65cfc0aefe1",
   "metadata": {
    "id": "0e3bf8a5-c6f8-4e16-89ea-b65cfc0aefe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 11:16:34,420] [INFO] [runner.py:378:main] Using IP address of 172.18.0.5 for node slurmnode1\n",
      "[2024-05-13 11:16:34,421] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2024-05-13 11:16:34,421] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_P2P_DISABLE=1; export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.5 --master_port=29500 /dli/minGPT/minGPT/runStep4.py --deepspeed --deepspeed_config '/dli/minGPT/minGPT/ds_config_step4.json'\n",
      "slurmnode2: [2024-05-13 11:16:36,659] [INFO] [launch.py:96:main] 1 NCCL_P2P_DISABLE=1\n",
      "slurmnode2: [2024-05-13 11:16:36,659] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2024-05-13 11:16:36,659] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2024-05-13 11:16:36,659] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2024-05-13 11:16:36,660] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2024-05-13 11:16:36,660] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2024-05-13 11:16:36,660] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-05-13 11:16:36,681] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "slurmnode1: [2024-05-13 11:16:36,681] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2024-05-13 11:16:36,682] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2024-05-13 11:16:36,682] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2024-05-13 11:16:36,682] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2024-05-13 11:16:36,682] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2024-05-13 11:16:36,682] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-05-13 11:16:38,371] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: \n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-05-13 11:17:15,956] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode1: [2024-05-13 11:17:18,947] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Detected CUDA files, patching ldflags\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Building extension module fused_adam...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.09417057037353516 seconds\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.10219717025756836 seconds\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.10204672813415527 seconds\n",
      "slurmnode1: [2024-05-13 11:17:19,777] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.1020660400390625 seconds\n",
      "slurmnode1: [2024-05-13 11:17:19,785] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "slurmnode1: [2024-05-13 11:17:19,785] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "slurmnode1: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode1: Building extension module utils...\n",
      "slurmnode1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: [2024-05-13 11:17:19,801] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2024-05-13 11:17:19,801] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2024-05-13 11:17:19,801] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2024-05-13 11:17:19,801] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:17:19,802] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2024-05-13 11:17:19,802] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": false, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": false, \n",
      "slurmnode1:     \"cpu_checkpointing\": false, \n",
      "slurmnode1:     \"number_checkpoints\": 12, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": false, \n",
      "slurmnode1:     \"profile\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:17:19,802] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2024-05-13 11:17:19,802] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2024-05-13 11:17:19,802] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2024-05-13 11:17:19,803] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.0003}\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2024-05-13 11:17:19,804] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   train_batch_size ............. 512\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  128\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 0, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": false, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": null, \n",
      "slurmnode1:     \"offload_optimizer\": null, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+07, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "slurmnode1: [2024-05-13 11:17:19,805] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "slurmnode1: [2024-05-13 11:17:19,807] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 128, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.0003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1.0, \n",
      "slurmnode1:     \"activation_checkpointing\": {\n",
      "slurmnode1:         \"number_checkpoints\": 12\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"fp16\": {\n",
      "slurmnode1:         \"enabled\": true\n",
      "slurmnode1:     }\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: ninja: no work to do.\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.09203290939331055 seconds\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.10222244262695312 seconds\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.10229611396789551 seconds\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.10213994979858398 seconds\n",
      "slurmnode1: [2024-05-13 11:17:22,828] [INFO] [checkpointing.py:547:forward] Activation Checkpointing Information\n",
      "slurmnode1: [2024-05-13 11:17:22,829] [INFO] [checkpointing.py:548:forward] ----Partition Activations False, CPU CHECKPOINTING False\n",
      "slurmnode1: [2024-05-13 11:17:22,829] [INFO] [checkpointing.py:551:forward] ----contiguous Memory Checkpointing False with None total layers\n",
      "slurmnode1: [2024-05-13 11:17:22,829] [INFO] [checkpointing.py:554:forward] ----Synchronization False\n",
      "slurmnode1: [2024-05-13 11:17:22,829] [INFO] [checkpointing.py:555:forward] ----Profiling time in checkpointing False\n",
      "slurmnode2: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 0\n",
      "slurmnode2: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode2: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 0\n",
      "slurmnode2: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode1: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 0\n",
      "slurmnode1: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode1: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 0\n",
      "slurmnode1: [2024-05-13 11:17:25,761] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n",
      "slurmnode1: [2024-05-13 11:17:25,761] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n",
      "slurmnode2: [2024-05-13 11:17:28,001] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 1\n",
      "slurmnode2: [2024-05-13 11:17:28,001] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode2: [2024-05-13 11:17:28,002] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 1\n",
      "slurmnode2: [2024-05-13 11:17:28,002] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode1: [2024-05-13 11:17:28,002] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 1\n",
      "slurmnode1: [2024-05-13 11:17:28,002] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode1: [2024-05-13 11:17:28,002] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 1\n",
      "slurmnode1: [2024-05-13 11:17:28,002] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n",
      "slurmnode1: [2024-05-13 11:17:28,002] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n",
      "slurmnode2: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 2\n",
      "slurmnode2: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode2: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 2\n",
      "slurmnode1: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 2\n",
      "slurmnode1: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode2: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode1: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 2\n",
      "slurmnode1: [2024-05-13 11:17:30,242] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n",
      "slurmnode1: [2024-05-13 11:17:30,243] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n",
      "slurmnode2: [2024-05-13 11:17:32,482] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 3\n",
      "slurmnode2: [2024-05-13 11:17:32,482] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode2: [2024-05-13 11:17:32,482] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 3\n",
      "slurmnode1: [2024-05-13 11:17:32,482] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 3\n",
      "slurmnode1: [2024-05-13 11:17:32,482] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode2: [2024-05-13 11:17:32,483] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode1: [2024-05-13 11:17:32,483] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 3\n",
      "slurmnode1: [2024-05-13 11:17:32,483] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n",
      "slurmnode1: [2024-05-13 11:17:32,483] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n",
      "slurmnode2: [2024-05-13 11:17:34,723] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 4\n",
      "slurmnode2: [2024-05-13 11:17:34,723] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode2: [2024-05-13 11:17:34,723] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 4\n",
      "slurmnode2: [2024-05-13 11:17:34,723] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode1: [2024-05-13 11:17:34,723] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 4\n",
      "slurmnode1: [2024-05-13 11:17:34,724] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode1: [2024-05-13 11:17:34,724] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 4\n",
      "slurmnode1: [2024-05-13 11:17:34,724] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n",
      "slurmnode1: [2024-05-13 11:17:34,724] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\n",
      "slurmnode1: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 5\n",
      "slurmnode1: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 5\n",
      "slurmnode1: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode1: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode2: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 5\n",
      "slurmnode2: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode1: [2024-05-13 11:17:36,966] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\n",
      "slurmnode2: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 5\n",
      "slurmnode2: [2024-05-13 11:17:36,966] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n",
      "slurmnode1: [2024-05-13 11:17:39,206] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 6\n",
      "slurmnode1: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 6\n",
      "slurmnode2: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 6\n",
      "slurmnode2: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode2: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 6\n",
      "slurmnode1: [2024-05-13 11:17:39,207] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0\n",
      "slurmnode2: [2024-05-13 11:17:39,207] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n",
      "slurmnode1: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 7\n",
      "slurmnode1: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode1: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 7\n",
      "slurmnode1: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode2: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 7\n",
      "slurmnode2: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode2: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 7\n",
      "slurmnode2: [2024-05-13 11:17:41,448] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n",
      "slurmnode1: [2024-05-13 11:17:41,448] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0\n",
      "slurmnode1: [2024-05-13 11:17:43,688] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 8\n",
      "slurmnode1: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode1: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 8\n",
      "slurmnode2: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 8\n",
      "slurmnode1: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode2: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode2: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 8\n",
      "slurmnode2: [2024-05-13 11:17:43,689] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n",
      "slurmnode1: [2024-05-13 11:17:43,689] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0\n",
      "slurmnode1: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 9\n",
      "slurmnode1: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode2: Grad overflow on iteration 9\n",
      "slurmnode2: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 9\n",
      "slurmnode2: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode2: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode1: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 9\n",
      "slurmnode1: [2024-05-13 11:17:45,930] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n",
      "slurmnode1: [2024-05-13 11:17:45,930] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0\n",
      "slurmnode1: [2024-05-13 11:17:45,931] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=10, lr=[6.912e-05], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:17:45,931] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=228.54136465908132, MemAllocated=0.28GB, MaxMemAllocated=9.49GB\n",
      "slurmnode2: [2024-05-13 11:17:48,177] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 10\n",
      "slurmnode2: [2024-05-13 11:17:48,177] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 10\n",
      "slurmnode2: [2024-05-13 11:17:48,178] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode2: [2024-05-13 11:17:48,178] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode1: [2024-05-13 11:17:48,178] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 10\n",
      "slurmnode1: [2024-05-13 11:17:48,178] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode1: [2024-05-13 11:17:48,178] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 10\n",
      "slurmnode1: [2024-05-13 11:17:48,178] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n",
      "slurmnode1: [2024-05-13 11:17:48,178] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0\n",
      "slurmnode2: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 11\n",
      "slurmnode2: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode2: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 11\n",
      "slurmnode2: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode1: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 11\n",
      "slurmnode1: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode1: [2024-05-13 11:17:50,422] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 11\n",
      "slurmnode1: [2024-05-13 11:17:50,423] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n",
      "slurmnode1: [2024-05-13 11:17:50,423] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0\n",
      "slurmnode2: [2024-05-13 11:17:52,666] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 12\n",
      "slurmnode2: [2024-05-13 11:17:52,666] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 12\n",
      "slurmnode2: [2024-05-13 11:17:52,666] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode2: [2024-05-13 11:17:52,666] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode1: [2024-05-13 11:17:52,666] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 12\n",
      "slurmnode1: [2024-05-13 11:17:52,667] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode1: [2024-05-13 11:17:52,667] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 12\n",
      "slurmnode1: [2024-05-13 11:17:52,667] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "slurmnode1: [2024-05-13 11:17:52,667] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n",
      "slurmnode1: [2024-05-13 11:17:54,912] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 13\n",
      "slurmnode1: [2024-05-13 11:17:54,912] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode1: [2024-05-13 11:17:54,912] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 13\n",
      "slurmnode1: [2024-05-13 11:17:54,913] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode1: [2024-05-13 11:17:54,913] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0\n",
      "slurmnode2: [2024-05-13 11:17:54,913] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 13\n",
      "slurmnode2: [2024-05-13 11:17:54,913] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode2: [2024-05-13 11:17:54,913] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 13\n",
      "slurmnode2: [2024-05-13 11:17:54,913] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n",
      "slurmnode1: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 14\n",
      "slurmnode1: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode1: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 14\n",
      "slurmnode1: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode1: [2024-05-13 11:17:57,154] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n",
      "slurmnode2: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 14\n",
      "slurmnode2: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode2: [2024-05-13 11:17:57,154] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 14\n",
      "slurmnode2: [2024-05-13 11:17:57,155] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n",
      "slurmnode2: [2024-05-13 11:17:59,395] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 15\n",
      "slurmnode2: [2024-05-13 11:17:59,396] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 15\n",
      "slurmnode2: [2024-05-13 11:17:59,396] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0\n",
      "slurmnode1: [2024-05-13 11:17:59,395] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: [2024-05-13 11:17:59,396] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0\n",
      "slurmnode1: Grad overflow on iteration 15\n",
      "slurmnode1: [2024-05-13 11:17:59,396] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0\n",
      "slurmnode1: [2024-05-13 11:17:59,396] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 15\n",
      "slurmnode1: [2024-05-13 11:17:59,396] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0\n",
      "slurmnode1: [2024-05-13 11:17:59,396] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0\n",
      "slurmnode1: [2024-05-13 11:18:01,637] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 16\n",
      "slurmnode1: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n",
      "slurmnode1: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode1: Grad overflow on iteration 16\n",
      "slurmnode1: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n",
      "slurmnode1: [2024-05-13 11:18:01,638] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n",
      "slurmnode2: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 16\n",
      "slurmnode2: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n",
      "slurmnode2: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "slurmnode2: Grad overflow on iteration 16\n",
      "slurmnode2: [2024-05-13 11:18:01,638] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n",
      "slurmnode1: [2024-05-13 11:18:08,430] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=17, lr=[0.00014592], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:18:08,430] [INFO] [timer.py:193:stop] 0/20, SamplesPerSec=228.03661986286824, MemAllocated=0.28GB, MaxMemAllocated=9.49GB\n",
      "slurmnode1: [2024-05-13 11:18:31,021] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=17, lr=[0.00022272], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:18:31,022] [INFO] [timer.py:193:stop] 0/30, SamplesPerSec=227.5569334298599, MemAllocated=0.28GB, MaxMemAllocated=9.49GB\n",
      "slurmnode1: [2024-05-13 11:18:53,609] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=17, lr=[0.00029952], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:18:53,609] [INFO] [timer.py:193:stop] 0/40, SamplesPerSec=227.3417796116267, MemAllocated=0.28GB, MaxMemAllocated=9.49GB\n",
      "slurmnode1: [2024-05-13 11:19:16,209] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=17, lr=[0.00037632], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:19:16,210] [INFO] [timer.py:193:stop] 0/50, SamplesPerSec=227.1887212539171, MemAllocated=0.28GB, MaxMemAllocated=9.49GB\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=4;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f84c360-9eb9-41aa-ab6c-b1eaec2add62",
   "metadata": {
    "id": "7f84c360-9eb9-41aa-ab6c-b1eaec2add62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode2: 05/13/2024 11:17:14 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 05/13/2024 11:17:15 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 05/13/2024 11:17:15 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 05/13/2024 11:17:16 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 05/13/2024 11:17:17 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "slurmnode2: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "slurmnode1: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode1: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode1: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 05/13/2024 11:17:18 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=4;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e2925-d6f1-456b-bfa9-dbec7325df58",
   "metadata": {
    "id": "817e2925-d6f1-456b-bfa9-dbec7325df58"
   },
   "source": [
    "코드 실행이 만족스럽다면 다음 단계 이동을 위해 배치 작업의 실행을 취소하는 것을 잊지 마십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cefa60c4-e58f-435e-9960-64a8445308e9",
   "metadata": {
    "id": "cefa60c4-e58f-435e-9960-64a8445308e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 4  slurmpar dli_asse    admin  R       2:52      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5904dcb-3a59-43c4-8806-d5320324855b",
   "metadata": {
    "id": "a5904dcb-3a59-43c4-8806-d5320324855b"
   },
   "outputs": [],
   "source": [
    "!scancel 4 #PASTE_JOB_ID_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112d963",
   "metadata": {
    "id": "f112d963"
   },
   "source": [
    "### 추가적인 최적화 고려 사항\n",
    "\n",
    "훈련 과정에 참여하는 모든 워커는 동일한 출력을 생성합니다. 따라서 k-mean 값은 두 번 계산됩니다. \n",
    "k-mean 구현을 조정하여 한 번만 실행하고 결과를 모든 워커에 재배포할 수 있습니다. \n",
    "다음은 어떻게 실행할 수 있는 지에 대한 예시입니다.\n",
    "\n",
    "```import torch.distributed as dist\n",
    "def run_kmeans(x, ncluster, niter=8, rank, size):\n",
    "    print('KMeans executed on rank ', rank, ' Worlds size ', size)\n",
    "    N, D = x.size()\n",
    "    c = x[torch.randperm(N)[:ncluster]] # init clusters at random\n",
    "    c = c.cuda(args.local_rank) # move the tensor to the GPU for exchange\n",
    "    if rank == 0:\n",
    "        # Computing KMeans only on rank 0 \n",
    "        with torch.no_grad():\n",
    "            c = kmeans(x, ncluster, niter)\n",
    "    # We now have computed the clusters so can proceed to the exchange\n",
    "    dist.barrier()\n",
    "    print('Broadcasting')\n",
    "    dist.broadcast(C.cuda(args.local_rank), src=0)\n",
    "    c=c.cpu()\n",
    "    print('Rank ', rank, ' has data ', C.size())\n",
    "    return c\n",
    "\n",
    "C=run_kmeans(px, ncluster, niter=8, dist.get_rank(), dist.get_world_size())    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2bd1d-d375-4db3-815f-9ae64336805f",
   "metadata": {
    "id": "42e2bd1d-d375-4db3-815f-9ae64336805f"
   },
   "source": [
    "## 5단계: 스케일업\n",
    "\n",
    "이제 최소한의 기능이 구현되었으므로 훈련 작업을 확장해 보겠습니다. 평가에서 스케일업 부분은 모델을 상당히 크게 만들 것입니다. \n",
    "\n",
    "&nbsp; &nbsp; 1. 모델의 아키텍처를 확장합니다.   \n",
    "&nbsp; &nbsp; 2. 활성화 체크포인팅, FP16 트레이닝, ZeRO 옵티마이저를 활성화하는 DeepSeed 구성 파일을 만듭니다.     \n",
    "&nbsp; &nbsp; 3. sbatch 훈련 파일을 만들고 실행합니다.  \n",
    "\n",
    "### 1. 모델의 아키텍처를 확장합니다.\n",
    "트레이닝 스크립트를 수정하기 전에 수정할 복사본을 만드는 것부터 시작하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5161b46d-16d2-4666-ba8a-a687d1912f93",
   "metadata": {
    "id": "5161b46d-16d2-4666-ba8a-a687d1912f93"
   },
   "outputs": [],
   "source": [
    "!cp /dli/minGPT/minGPT/runFirstDeepSpeed.py /dli/minGPT/minGPT/runStep5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d6341-5b42-4d1f-a6cd-365e0bb4fbbf",
   "metadata": {
    "id": "2e2d6341-5b42-4d1f-a6cd-365e0bb4fbbf"
   },
   "source": [
    " [runStep5.py](./minGPT/minGPT/runStep5.py)에서  \"GPTConfig\" 섹션을 수정하여 VisionTransformers의 레이어 수를 **24** 로 조정합니다.  \"GPTConfig\" 에서는 신경망 차원의 아키텍처가 다음과 같이 정의됩니다.\n",
    "\n",
    "```\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  embd_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0,\n",
    "                  n_layer=12, n_head=8, n_embd=256)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. 활성화 체크포인팅, FP16 트레이닝, ZeRO 옵티마이저를 지원하는 DeepSeed 구성 파일을 만듭니다.\n",
    "\n",
    "[ds_config_step5.json](./minGPT/minGPT/ds_config_step5.json) 을 변경하여 다음을 사용하도록 재구성합니다.:\n",
    "- 그래디언트 누적 및 4단계 누적 단계를 실행하여 글로벌 배치 크기를 늘립니다 (고정된 하이퍼 파라미터를 유지하는 데 자주 필요합니다).\n",
    "- 12개의 체크포인트가 아닌 24개의 체크포인트를 만들기 위한 활성화 체크포인팅을 실행합니다.\n",
    "- FP16 트레이닝\n",
    "- 파라미터 및 옵티마이저 상태 모두에 대해 CPU 오프로드를 지원하는 Zero Stage 3 옵티마이저를 실행합니다. 자세한 내용은  [ZeRO 설명서](https://deepspeed.readthedocs.io/en/latest/zero3.html) 를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64ce4ee5-52a3-4611-9438-242b1b2837b2",
   "metadata": {
    "id": "64ce4ee5-52a3-4611-9438-242b1b2837b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minGPT/minGPT/ds_config_step5.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile minGPT/minGPT/ds_config_step5.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 128,\n",
    "  \"gradient_accumulation_steps\": 4,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "  \"activation_checkpointing\": {\n",
    "    \"number_checkpoints\": 24\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"offload_optimizer\": {\n",
    "        \"device\": \"cpu\"\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338d1e4-5814-48b6-935f-10a111f8f76e",
   "metadata": {
    "id": "4338d1e4-5814-48b6-935f-10a111f8f76e"
   },
   "source": [
    "### 3. sbatch 트레이닝 파일을 만들고 실행합니다. \n",
    "다음 셀을 실행하여 5단계 훈련에 대한 sbatch 스크립트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5b9051e-1386-4aff-be66-d16d637f9377",
   "metadata": {
    "id": "c5b9051e-1386-4aff-be66-d16d637f9377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep5.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep5.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step5\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runStep5.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/minGPT/minGPT/ds_config_step5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a41a70-bc32-439c-a9f2-8a9faf70fe6f",
   "metadata": {
    "id": "c9a41a70-bc32-439c-a9f2-8a9faf70fe6f"
   },
   "source": [
    "위의 내용을 변경한 후에는 다음 명령어를 사용하여 작업을 실행하십시오:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "40801a02-10c5-4dd6-be57-ed2a5ad24516",
   "metadata": {
    "id": "40801a02-10c5-4dd6-be57-ed2a5ad24516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 6\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 6  slurmpar dli_asse    admin  R       0:00      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!sbatch /dli/minGPT/minGPT/runSlurmStep5.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef1ba9-40e3-421d-b3d4-3bf9cb1350c9",
   "metadata": {
    "id": "2fef1ba9-40e3-421d-b3d4-3bf9cb1350c9"
   },
   "source": [
    "다음을 사용하여 코드가 실행되었는지 확인합니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb23a05d-cfe3-4049-b97b-c8370ed38799",
   "metadata": {
    "id": "cb23a05d-cfe3-4049-b97b-c8370ed38799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 11:32:24,912] [INFO] [runner.py:378:main] Using IP address of 172.18.0.5 for node slurmnode1\n",
      "[2024-05-13 11:32:24,913] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2024-05-13 11:32:24,913] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_P2P_DISABLE=1; export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.5 --master_port=29500 /dli/minGPT/minGPT/runStep5.py --deepspeed --deepspeed_config '/dli/minGPT/minGPT/ds_config_step5.json'\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:96:main] 1 NCCL_P2P_DISABLE=1\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2024-05-13 11:32:27,161] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-05-13 11:32:27,181] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "slurmnode1: [2024-05-13 11:32:27,181] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2024-05-13 11:32:27,182] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2024-05-13 11:32:27,182] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2024-05-13 11:32:27,182] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2024-05-13 11:32:27,182] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2024-05-13 11:32:27,182] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2024-05-13 11:32:28,882] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-05-13 11:33:06,181] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2024-05-13 11:33:09,874] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Detected CUDA files, patching ldflags\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/cpu_adam/build.ninja...\n",
      "slurmnode2: Building extension module cpu_adam...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module cpu_adam...\n",
      "slurmnode2: Time to load cpu_adam op: 2.6334803104400635 seconds\n",
      "slurmnode1: Loading extension module cpu_adam...\n",
      "slurmnode1: Time to load cpu_adam op: 2.5951807498931885 seconds\n",
      "slurmnode1: Loading extension module cpu_adam...\n",
      "slurmnode1: Time to load cpu_adam op: 2.6337451934814453 seconds\n",
      "slurmnode2: Loading extension module cpu_adam...\n",
      "slurmnode2: Time to load cpu_adam op: 2.7340152263641357 seconds\n",
      "slurmnode2: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode2: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode1: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode1: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode1: [2024-05-13 11:33:14,449] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode1: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode1: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode2: Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "slurmnode2: Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "slurmnode1: [2024-05-13 11:33:14,479] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "slurmnode1: [2024-05-13 11:33:14,480] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "slurmnode1: [2024-05-13 11:33:14,480] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "slurmnode1: [2024-05-13 11:33:14,480] [INFO] [engine.py:1410:_configure_zero_optimizer] Initializing ZeRO Stage 3\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: [2024-05-13 11:33:14,488] [INFO] [stage3.py:275:__init__] Reduce bucket size 500000000\n",
      "slurmnode1: [2024-05-13 11:33:14,488] [INFO] [stage3.py:276:__init__] Prefetch bucket size 50000000\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "slurmnode2: Building extension module utils...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.11379528045654297 seconds\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.10170197486877441 seconds\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.10160493850708008 seconds\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.20180130004882812 seconds\n",
      "slurmnode1: [2024-05-13 11:33:15,023] [INFO] [stage3.py:567:_setup_for_real_optimizer] optimizer state initialized\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.00043702125549316406 seconds\n",
      "slurmnode2: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode2: Loading extension module utils...\n",
      "slurmnode2: Time to load utils op: 0.00048732757568359375 seconds\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.0004222393035888672 seconds\n",
      "slurmnode1: [2024-05-13 11:33:15,537] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer\n",
      "slurmnode1: [2024-05-13 11:33:15,538] [INFO] [utils.py:829:see_memory_usage] MA 0.98 GB         Max_MA 0.98 GB         CA 1.03 GB         Max_CA 1 GB \n",
      "slurmnode1: [2024-05-13 11:33:15,539] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 17.63 GB, percent = 7.3%\n",
      "slurmnode1: [2024-05-13 11:33:15,539] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2024-05-13 11:33:15,539] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2024-05-13 11:33:15,539] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2024-05-13 11:33:15,539] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2024-05-13 11:33:15,540] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2024-05-13 11:33:15,541] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": false, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": false, \n",
      "slurmnode1:     \"cpu_checkpointing\": false, \n",
      "slurmnode1:     \"number_checkpoints\": 24, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": false, \n",
      "slurmnode1:     \"profile\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:33:15,541] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2024-05-13 11:33:15,541] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2024-05-13 11:33:15,541] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2024-05-13 11:33:15,541] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2024-05-13 11:33:15,542] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "slurmnode1: [2024-05-13 11:33:15,543] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 4\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.0003}\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2024-05-13 11:33:15,544] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   train_batch_size ............. 2048\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  128\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 3, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": true, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": {\n",
      "slurmnode1:         \"device\": \"cpu\", \n",
      "slurmnode1:         \"nvme_path\": null, \n",
      "slurmnode1:         \"buffer_count\": 5, \n",
      "slurmnode1:         \"buffer_size\": 1.000000e+08, \n",
      "slurmnode1:         \"max_in_cpu\": 1.000000e+09, \n",
      "slurmnode1:         \"pin_memory\": false\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"offload_optimizer\": {\n",
      "slurmnode1:         \"device\": \"cpu\", \n",
      "slurmnode1:         \"nvme_path\": null, \n",
      "slurmnode1:         \"buffer_count\": 4, \n",
      "slurmnode1:         \"pin_memory\": false, \n",
      "slurmnode1:         \"pipeline_read\": false, \n",
      "slurmnode1:         \"pipeline_write\": false, \n",
      "slurmnode1:         \"fast_init\": false, \n",
      "slurmnode1:         \"pipeline\": false\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+07, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   zero_enabled ................. True\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 3\n",
      "slurmnode1: [2024-05-13 11:33:15,545] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 128, \n",
      "slurmnode1:     \"gradient_accumulation_steps\": 4, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.0003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1.0, \n",
      "slurmnode1:     \"activation_checkpointing\": {\n",
      "slurmnode1:         \"number_checkpoints\": 24\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"fp16\": {\n",
      "slurmnode1:         \"enabled\": true\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"zero_optimization\": {\n",
      "slurmnode1:         \"stage\": 3, \n",
      "slurmnode1:         \"offload_optimizer\": {\n",
      "slurmnode1:             \"device\": \"cpu\"\n",
      "slurmnode1:         }, \n",
      "slurmnode1:         \"offload_param\": {\n",
      "slurmnode1:             \"device\": \"cpu\"\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "slurmnode1: Loading extension module utils...\n",
      "slurmnode1: Time to load utils op: 0.0004458427429199219 seconds\n",
      "slurmnode1: [2024-05-13 11:33:18,025] [INFO] [checkpointing.py:547:forward] Activation Checkpointing Information\n",
      "slurmnode1: [2024-05-13 11:33:18,026] [INFO] [checkpointing.py:548:forward] ----Partition Activations False, CPU CHECKPOINTING False\n",
      "slurmnode1: [2024-05-13 11:33:18,026] [INFO] [checkpointing.py:551:forward] ----contiguous Memory Checkpointing False with None total layers\n",
      "slurmnode1: [2024-05-13 11:33:18,026] [INFO] [checkpointing.py:554:forward] ----Synchronization False\n",
      "slurmnode1: [2024-05-13 11:33:18,026] [INFO] [checkpointing.py:555:forward] ----Profiling time in checkpointing False\n",
      "slurmnode1: [2024-05-13 11:33:38,904] [INFO] [stage3.py:2281:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n",
      "slurmnode1: [2024-05-13 11:33:57,637] [INFO] [stage3.py:2281:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n",
      "slurmnode1: [2024-05-13 11:34:06,903] [INFO] [timer.py:193:stop] 0/10, SamplesPerSec=107.33073894177907, MemAllocated=1.12GB, MaxMemAllocated=11.1GB\n",
      "slurmnode1: [2024-05-13 11:34:16,529] [INFO] [stage3.py:2281:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n",
      "slurmnode1: [2024-05-13 11:34:35,261] [INFO] [stage3.py:2281:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=6;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c41933e8-7789-495a-8b97-3be96ff8f99a",
   "metadata": {
    "id": "c41933e8-7789-495a-8b97-3be96ff8f99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1: 05/13/2024 11:33:04 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode2: 05/13/2024 11:33:05 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode1: 05/13/2024 11:33:06 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode2: 05/13/2024 11:33:07 - INFO - mingpt.model -   number of parameters: 1.947878e+07\n",
      "slurmnode1: 05/13/2024 11:33:08 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode1: 05/13/2024 11:33:08 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode2: 05/13/2024 11:33:08 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "slurmnode2: 05/13/2024 11:33:09 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "slurmnode2: 05/13/2024 11:33:09 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 05/13/2024 11:33:09 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 05/13/2024 11:33:09 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 05/13/2024 11:33:09 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=6;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae45e4-bef1-4394-b448-5162d9d4b8cb",
   "metadata": {
    "id": "1bae45e4-bef1-4394-b448-5162d9d4b8cb"
   },
   "source": [
    "다음 단계로 이동하기 전에 실행 및 보류 중인 모든 작업을 중지해야합니다. 아니면 평가 결과 Fail을 받을 것입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b15fb509-ab9e-4d39-95cd-28351033d830",
   "metadata": {
    "id": "b15fb509-ab9e-4d39-95cd-28351033d830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                 6  slurmpar dli_asse    admin  R       4:57      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d58077c-c4d4-46fa-b4c4-1d9442eb14c9",
   "metadata": {
    "id": "3d58077c-c4d4-46fa-b4c4-1d9442eb14c9"
   },
   "outputs": [],
   "source": [
    "!scancel 6 #PASTE_JOB_ID_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4db940-8ab3-4282-91c6-d631dfbf571a",
   "metadata": {
    "id": "0a4db940-8ab3-4282-91c6-d631dfbf571a"
   },
   "source": [
    "## 6단계: 평가\n",
    "\n",
    "위에 나열된 변경 사항을 모두 구현한 경우 5단계에서 확인한 작업 ID를 아래 코드 블록에 입력하십시오. 문제가 올바르게 완료된 경우 \"Assessment Passed!\" 메시지가 나타납니다. 행운을 빌겠습니다! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b806b2b0-a877-4c02-a88b-e2cb944be111",
   "metadata": {
    "id": "b806b2b0-a877-4c02-a88b-e2cb944be111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Job ID: 6\n",
      "Assessment Passed! Congratulations!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "from run_assessment import run_assessment\n",
    "job_id = 6 #PASTE_JOB_ID_HERE\n",
    "run_assessment(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c537006-d407-44b1-8741-1531bfc8609b",
   "metadata": {
    "id": "0c537006-d407-44b1-8741-1531bfc8609b"
   },
   "source": [
    "\"Assessment Passed!\"가 나타나면 DLI 포털로 돌아가서 평가(assess) 버튼을 누르시면 인증서가 생성됩니다. 성공적으로 과정을 마무리하셨습니다! 축하합니다!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
